{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Megakernel for Qwen3-0.6B\n",
    "\n",
    "A single CUDA kernel that fuses the entire Qwen3-0.6B decode forward pass.\n",
    "Targeting **1,200+ tok/s on A100** and **2,600+ tok/s on H100**.\n",
    "\n",
    "**Architecture**: Non-cooperative persistent kernel with:\n",
    "- Atomic barriers (not cooperative `grid.sync()`)\n",
    "- Productive spin: idle blocks prefetch weights during attention\n",
    "- Redundant RMSNorm to eliminate barriers\n",
    "- On-device argmax sampling (no CPU readback)\n",
    "- 128-bit vectorized weight loads with L1 bypass\n",
    "\n",
    "**References**:\n",
    "- [MegaQwen](https://elliotarledge.com/blog/megaqwen) (530 tok/s, RTX 3090)\n",
    "- [1k tok/s kernel](https://blog.alpindale.net/posts/5090_decode_optimization/) (RTX 5090)\n",
    "- [Hazy Research Megakernels](https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles) (<1ms, H100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q transformers torch safetensors accelerate\n\n# Check GPU\n!nvidia-smi\n\nimport torch\nimport os\nimport subprocess\nimport time\nimport json\nimport numpy as np\n\n# GPU detection\ndevice = torch.cuda.current_device()\nprops = torch.cuda.get_device_properties(device)\ncc = props.major * 10 + props.minor\n\nGPU_NAME = props.name\nSM_COUNT = props.multi_processor_count\nTOTAL_MEM_GB = getattr(props, 'total_global_memory', getattr(props, 'total_mem', 0)) / 1e9\n\nif cc >= 90:\n    ARCH = \"sm_90\"\n    GPU_CLASS = \"H100\"\n    PEAK_BW = 3350.0\nelif cc >= 80:\n    ARCH = \"sm_80\"\n    GPU_CLASS = \"A100\"\n    PEAK_BW = 2039.0 if TOTAL_MEM_GB > 50 else 1555.0\nelif cc >= 75:\n    ARCH = \"sm_75\"\n    GPU_CLASS = \"T4\"\n    PEAK_BW = 300.0\nelse:\n    ARCH = f\"sm_{cc}\"\n    GPU_CLASS = \"Unknown\"\n    PEAK_BW = 200.0\n\n# Actual model size: 751M params * 2 bytes (BF16) = ~1.5 GB\nMODEL_BYTES_BF16 = 1503e6\nTHEORETICAL_MAX = PEAK_BW * 1e9 / MODEL_BYTES_BF16\n\nprint(f\"GPU: {GPU_NAME}\")\nprint(f\"Class: {GPU_CLASS} ({ARCH})\")\nprint(f\"SMs: {SM_COUNT}\")\nprint(f\"Memory: {TOTAL_MEM_GB:.1f} GB\")\nprint(f\"Peak Bandwidth: {PEAK_BW:.0f} GB/s\")\nprint(f\"Theoretical max tok/s (Qwen3-0.6B BF16): {THEORETICAL_MAX:.0f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Download & Convert Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_ID = \"Qwen/Qwen3-0.6B\"\nCACHE_DIR = \"./model_cache\"\n\nprint(f\"Downloading {MODEL_ID}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cpu\",  # load to CPU first, we'll manage GPU memory\n    cache_dir=CACHE_DIR\n)\nprint(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n\n# Verify architecture matches our expectations\ncfg = model.config\nassert cfg.hidden_size == 1024, f\"Expected hidden_size=1024, got {cfg.hidden_size}\"\nassert cfg.num_hidden_layers == 28, f\"Expected 28 layers, got {cfg.num_hidden_layers}\"\nassert cfg.num_attention_heads == 16, f\"Expected 16 Q heads, got {cfg.num_attention_heads}\"\nassert cfg.num_key_value_heads == 8, f\"Expected 8 KV heads, got {cfg.num_key_value_heads}\"\n\nrope_theta = getattr(cfg, 'rope_theta', None)\nif rope_theta is None and hasattr(cfg, 'rope_parameters'):\n    rope_theta = cfg.rope_parameters.get('rope_theta', 1000000)\n\nprint(\"\\nArchitecture verified!\")\nprint(f\"  Hidden dim: {cfg.hidden_size}\")\nprint(f\"  Layers: {cfg.num_hidden_layers}\")\nprint(f\"  Q heads: {cfg.num_attention_heads}, KV heads: {cfg.num_key_value_heads}\")\nprint(f\"  Head dim: {cfg.head_dim}\")\nprint(f\"  Intermediate size: {cfg.intermediate_size}\")\nprint(f\"  Vocab size: {cfg.vocab_size}\")\nprint(f\"  RoPE theta: {rope_theta}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert weights to flat binary format\nimport struct\n\n# Use dimensions from the actual model config (not hardcoded)\nHIDDEN_DIM = cfg.hidden_size          # 1024\nNUM_LAYERS = cfg.num_hidden_layers    # 28\nNUM_Q_HEADS = cfg.num_attention_heads # 16\nNUM_KV_HEADS = cfg.num_key_value_heads # 8\nHEAD_DIM = cfg.head_dim               # 128\nQ_DIM = NUM_Q_HEADS * HEAD_DIM       # 2048\nKV_DIM = NUM_KV_HEADS * HEAD_DIM     # 1024\nINTERMEDIATE_DIM = cfg.intermediate_size  # 3072\nVOCAB_SIZE = cfg.vocab_size           # 151936\n\nprint(f\"Dimensions: HEAD_DIM={HEAD_DIM}, Q_DIM={Q_DIM}, KV_DIM={KV_DIM}, INTERMEDIATE_DIM={INTERMEDIATE_DIM}\")\n\nstate = model.state_dict()\n\n# Extract and save weights in our flat binary format\nparts = []\noffsets = {}\npos = 0\n\ndef add(name, tensor):\n    global pos\n    t = tensor.contiguous().to(torch.bfloat16)\n    data = t.view(torch.uint16).numpy().tobytes()\n    offsets[name] = pos\n    parts.append(data)\n    pos += len(data)\n\nprint(\"Packing weights...\")\n\n# Embedding\nadd(\"embedding\", state[\"model.embed_tokens.weight\"])\nprint(f\"  embedding: {state['model.embed_tokens.weight'].shape}\")\n\n# Final norm\nadd(\"final_norm\", state[\"model.norm.weight\"])\n\n# Per-layer (11 weights each: attn_norm, w_q, w_k, w_v, q_norm, k_norm, w_o, ffn_norm, w_gate, w_up, w_down)\nfor i in range(NUM_LAYERS):\n    prefix = f\"model.layers.{i}\"\n    add(f\"layer.{i}.attn_norm\", state[f\"{prefix}.input_layernorm.weight\"])\n    add(f\"layer.{i}.w_q\", state[f\"{prefix}.self_attn.q_proj.weight\"])\n    add(f\"layer.{i}.w_k\", state[f\"{prefix}.self_attn.k_proj.weight\"])\n    add(f\"layer.{i}.w_v\", state[f\"{prefix}.self_attn.v_proj.weight\"])\n    add(f\"layer.{i}.q_norm\", state[f\"{prefix}.self_attn.q_norm.weight\"])\n    add(f\"layer.{i}.k_norm\", state[f\"{prefix}.self_attn.k_norm.weight\"])\n    add(f\"layer.{i}.w_o\", state[f\"{prefix}.self_attn.o_proj.weight\"])\n    add(f\"layer.{i}.ffn_norm\", state[f\"{prefix}.post_attention_layernorm.weight\"])\n    add(f\"layer.{i}.w_gate\", state[f\"{prefix}.mlp.gate_proj.weight\"])\n    add(f\"layer.{i}.w_up\", state[f\"{prefix}.mlp.up_proj.weight\"])\n    add(f\"layer.{i}.w_down\", state[f\"{prefix}.mlp.down_proj.weight\"])\n\nprint(f\"  {NUM_LAYERS} layers packed (11 weights each)\")\n\n# Save binary\nweights_path = \"weights.bin\"\nwith open(weights_path, \"wb\") as f:\n    f.write(b\"\".join(parts))\n\noffsets_path = \"weights_offsets.json\"\nwith open(offsets_path, \"w\") as f:\n    json.dump(offsets, f, indent=2)\n\ntotal_mb = pos / 1e6\nprint(f\"\\nSaved {total_mb:.1f} MB to {weights_path}\")\nprint(f\"Offsets saved to {offsets_path}\")\nprint(f\"Total offsets: {len(offsets)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Compile Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if source files exist\nimport os\nif not os.path.exists(\"src/megakernel.cu\"):\n    print(\"Source files not found! Please clone the repo first:\")\n    print(\"  !git clone https://github.com/MaruthiV/megakernel.git\")\n    print(\"  Then cd into the repo directory.\")\nelse:\n    print(\"Source files found.\")\n\n# Compile standalone binary (for info/testing)\nprint(f\"\\nCompiling standalone for {GPU_CLASS} ({ARCH})...\")\nresult = subprocess.run(\n    [\"nvcc\", \"-O3\", f\"-arch={ARCH}\", \"-std=c++17\", \"--use_fast_math\",\n     \"-lineinfo\", \"-Isrc\", \"src/megakernel.cu\", \"-o\", \"megakernel\"],\n    capture_output=True, text=True\n)\n\nif result.returncode == 0:\n    print(\"Standalone binary: OK\")\nelse:\n    print(f\"Standalone compilation FAILED!\\n{result.stderr}\")\n\n# Compile shared library (for Python ctypes bridge)\nprint(f\"Compiling shared library for {GPU_CLASS} ({ARCH})...\")\nresult_lib = subprocess.run(\n    [\"nvcc\", \"-O3\", f\"-arch={ARCH}\", \"-std=c++17\", \"--use_fast_math\",\n     \"-shared\", \"-Xcompiler\", \"-fPIC\", \"-DMEGAKERNEL_LIBRARY_MODE\",\n     \"-lineinfo\", \"-Isrc\", \"src/megakernel.cu\", \"-o\", \"megakernel.so\"],\n    capture_output=True, text=True\n)\n\nif result_lib.returncode == 0:\n    print(\"Shared library: OK\")\nelse:\n    print(f\"Library compilation FAILED!\\n{result_lib.stderr}\")\n\n# Run standalone info\nif result.returncode == 0:\n    print(\"\\n--- Kernel Info ---\")\n    !./megakernel"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Correctness Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PyTorch reference forward pass\n",
    "model_gpu = model.to(\"cuda\")\n",
    "model_gpu.eval()\n",
    "\n",
    "# Reference: generate 50 tokens with greedy decoding\n",
    "prompt = \"The meaning of life is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Input IDs: {input_ids[0].tolist()[:10]}...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    ref_output = model_gpu.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "ref_text = tokenizer.decode(ref_output[0], skip_special_tokens=True)\n",
    "ref_tokens = ref_output[0].tolist()\n",
    "print(f\"\\nReference output ({len(ref_tokens)} tokens):\")\n",
    "print(f\"  {ref_text[:300]}\")\n",
    "\n",
    "# Get reference logits for first token\n",
    "with torch.no_grad():\n",
    "    ref_logits = model_gpu(input_ids).logits[0, -1, :]  # [VOCAB_SIZE]\n",
    "    ref_next_token = ref_logits.argmax().item()\n",
    "    print(f\"\\nReference next token: {ref_next_token} = '{tokenizer.decode([ref_next_token])}'\")\n",
    "    print(f\"Top-5 logits: {ref_logits.topk(5)}\")\n",
    "\n",
    "# TODO: Compare with megakernel output once the kernel produces correct results\n",
    "print(\"\\n[Megakernel validation will be added once kernel is verified]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: HuggingFace Baseline Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure HuggingFace baseline throughput\n",
    "print(\"Measuring HuggingFace baseline throughput...\")\n",
    "\n",
    "# Short context (position ~1)\n",
    "input_ids = torch.tensor([[1]], device=\"cuda\")\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(input_ids)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "N = 50\n",
    "start = time.perf_counter()\n",
    "for _ in range(N):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(input_ids)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "hf_toks = N / elapsed\n",
    "\n",
    "print(f\"\\nHuggingFace baseline (short context): {hf_toks:.1f} tok/s\")\n",
    "print(f\"Per-token latency: {1e6/hf_toks:.1f} us\")\n",
    "\n",
    "# Long context baseline\n",
    "long_input = torch.randint(0, cfg.vocab_size, (1, 1024), device=\"cuda\")\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(long_input)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "N_long = 20\n",
    "start = time.perf_counter()\n",
    "for _ in range(N_long):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(long_input)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "hf_long_toks = N_long / elapsed\n",
    "\n",
    "print(f\"HuggingFace baseline (1024 context): {hf_long_toks:.1f} tok/s\")\n",
    "\n",
    "HF_BASELINE = hf_toks\n",
    "print(f\"\\nBaseline to beat: {HF_BASELINE:.1f} tok/s\")\n",
    "print(f\"Target: {THEORETICAL_MAX * 0.71:.0f}+ tok/s (71% bandwidth utilization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Megakernel Benchmarks\n",
    "\n",
    "Once the kernel is fully working, this cell runs the full benchmark suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize megakernel engine\nimport sys\nsys.path.insert(0, \".\")\nfrom host.bridge import MegakernelEngine\n\nengine = MegakernelEngine(\n    weights_path=\"weights.bin\",\n    offsets_path=\"weights_offsets.json\",\n    lib_path=\"./megakernel.so\",\n    tokenizer=tokenizer,\n)\n\n# Benchmark sweep across context positions\nprint(\"=\" * 70)\nprint(\"  MEGAKERNEL BENCHMARK\")\nprint(\"=\" * 70)\nprint(f\"  GPU:             {GPU_NAME}\")\nprint(f\"  Peak BW:         {PEAK_BW:.0f} GB/s\")\nprint(f\"  Theoretical max: {THEORETICAL_MAX:.0f} tok/s\")\nprint(f\"  HF baseline:     {HF_BASELINE:.1f} tok/s\")\nprint(\"=\" * 70)\nprint()\n\nresults = engine.benchmark_sweep()\n\nprint(f\"{'Position':>10} {'Tok/s':>10} {'Latency(us)':>12} {'BW(GB/s)':>10} {'BW Util%':>10} {'vs HF':>8}\")\nprint(\"-\" * 62)\nfor r in results:\n    bw_util = r[\"bw_gbps\"] / PEAK_BW * 100\n    speedup = r[\"tok_per_sec\"] / HF_BASELINE if HF_BASELINE > 0 else 0\n    print(f\"{r['position']:>10} {r['tok_per_sec']:>10.1f} {r['latency_us']:>12.1f} \"\n          f\"{r['bw_gbps']:>10.1f} {bw_util:>9.1f}% {speedup:>7.1f}x\")\n\n# Peak result\nbest = max(results, key=lambda r: r[\"tok_per_sec\"])\nbest_util = best[\"bw_gbps\"] / PEAK_BW * 100\nbest_speedup = best[\"tok_per_sec\"] / HF_BASELINE if HF_BASELINE > 0 else 0\nprint()\nprint(f\"  PEAK: {best['tok_per_sec']:.0f} tok/s \"\n      f\"({best_util:.1f}% BW, {best_speedup:.1f}x vs HuggingFace)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Text Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive text generation with the megakernel\nprompt = input(\"Enter your prompt: \")\n\nprint(f\"\\nPrompt: {prompt}\")\nprint(\"Generating...\\n\")\n\ntext, tok_per_sec = engine.generate(prompt, max_tokens=200, print_stream=True)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Throughput: {tok_per_sec:.1f} tok/s\")\nprint(f\"Latency:   {1e6/tok_per_sec:.0f} us/token\")\n\nbw_gbps = MODEL_BYTES_BF16 * tok_per_sec / 1e9\nbw_util = bw_gbps / PEAK_BW * 100\nprint(f\"Bandwidth: {bw_gbps:.1f} GB/s ({bw_util:.1f}% utilization)\")\nif HF_BASELINE > 0:\n    print(f\"Speedup:   {tok_per_sec/HF_BASELINE:.1f}x vs HuggingFace\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Profiling (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Profile with nsys\n",
    "# Requires nsight-systems to be installed (available on Colab)\n",
    "\n",
    "print(\"Profiling is optional. Uncomment to run:\")\n",
    "# !nsys profile --stats=true --output=megakernel_profile ./megakernel\n",
    "# !nsys stats megakernel_profile.nsys-rep\n",
    "\n",
    "# For detailed kernel analysis:\n",
    "# !ncu --set full --launch-count 1 ./megakernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Metric | HuggingFace | MegaQwen | Our Kernel | Theoretical Max |\n",
    "|--------|-------------|----------|------------|------------------|\n",
    "| Tok/s (short ctx) | ~136 | ~530 | TBD | See above |\n",
    "| Tok/s (long ctx) | ~59 | ~158 | TBD | - |\n",
    "| BW Utilization | ~5% | ~5% | Target 71%+ | 100% |\n",
    "| GPU | RTX 3090 | RTX 3090 | A100/H100 | - |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}