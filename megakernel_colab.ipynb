{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Megakernel for Qwen3-0.6B\n",
    "\n",
    "A single CUDA kernel that fuses the entire Qwen3-0.6B decode forward pass.\n",
    "Targeting **1,200+ tok/s on A100** and **2,600+ tok/s on H100**.\n",
    "\n",
    "**Architecture**: Non-cooperative persistent kernel with:\n",
    "- Atomic barriers (not cooperative `grid.sync()`)\n",
    "- Productive spin: idle blocks prefetch weights during attention\n",
    "- Redundant RMSNorm to eliminate barriers\n",
    "- On-device argmax sampling (no CPU readback)\n",
    "- 128-bit vectorized weight loads with L1 bypass\n",
    "\n",
    "**References**:\n",
    "- [MegaQwen](https://elliotarledge.com/blog/megaqwen) (530 tok/s, RTX 3090)\n",
    "- [1k tok/s kernel](https://blog.alpindale.net/posts/5090_decode_optimization/) (RTX 5090)\n",
    "- [Hazy Research Megakernels](https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles) (<1ms, H100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch safetensors accelerate\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# GPU detection\n",
    "device = torch.cuda.current_device()\n",
    "props = torch.cuda.get_device_properties(device)\n",
    "cc = props.major * 10 + props.minor\n",
    "\n",
    "GPU_NAME = props.name\n",
    "SM_COUNT = props.multi_processor_count\n",
    "TOTAL_MEM_GB = props.total_mem / 1e9\n",
    "\n",
    "if cc >= 90:\n",
    "    ARCH = \"sm_90\"\n",
    "    GPU_CLASS = \"H100\"\n",
    "    PEAK_BW = 3350.0\n",
    "elif cc >= 80:\n",
    "    ARCH = \"sm_80\"\n",
    "    GPU_CLASS = \"A100\"\n",
    "    PEAK_BW = 2039.0 if TOTAL_MEM_GB > 50 else 1555.0\n",
    "elif cc >= 75:\n",
    "    ARCH = \"sm_75\"\n",
    "    GPU_CLASS = \"T4\"\n",
    "    PEAK_BW = 300.0\n",
    "else:\n",
    "    ARCH = f\"sm_{cc}\"\n",
    "    GPU_CLASS = \"Unknown\"\n",
    "    PEAK_BW = 200.0\n",
    "\n",
    "MODEL_BYTES_BF16 = 926e6  # ~926 MB\n",
    "THEORETICAL_MAX = PEAK_BW * 1e9 / MODEL_BYTES_BF16\n",
    "\n",
    "print(f\"GPU: {GPU_NAME}\")\n",
    "print(f\"Class: {GPU_CLASS} ({ARCH})\")\n",
    "print(f\"SMs: {SM_COUNT}\")\n",
    "print(f\"Memory: {TOTAL_MEM_GB:.1f} GB\")\n",
    "print(f\"Peak Bandwidth: {PEAK_BW:.0f} GB/s\")\n",
    "print(f\"Theoretical max tok/s (Qwen3-0.6B BF16): {THEORETICAL_MAX:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Download & Convert Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "CACHE_DIR = \"./model_cache\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # load to CPU first, we'll manage GPU memory\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "print(f\"Model loaded. Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Config: {model.config}\")\n",
    "\n",
    "# Verify architecture matches our expectations\n",
    "cfg = model.config\n",
    "assert cfg.hidden_size == 1024, f\"Expected hidden_size=1024, got {cfg.hidden_size}\"\n",
    "assert cfg.num_hidden_layers == 28, f\"Expected 28 layers, got {cfg.num_hidden_layers}\"\n",
    "assert cfg.num_attention_heads == 16, f\"Expected 16 Q heads, got {cfg.num_attention_heads}\"\n",
    "assert cfg.num_key_value_heads == 8, f\"Expected 8 KV heads, got {cfg.num_key_value_heads}\"\n",
    "print(\"\\nArchitecture verified!\")\n",
    "print(f\"  Hidden dim: {cfg.hidden_size}\")\n",
    "print(f\"  Layers: {cfg.num_hidden_layers}\")\n",
    "print(f\"  Q heads: {cfg.num_attention_heads}, KV heads: {cfg.num_key_value_heads}\")\n",
    "print(f\"  Intermediate size: {cfg.intermediate_size}\")\n",
    "print(f\"  Vocab size: {cfg.vocab_size}\")\n",
    "print(f\"  RoPE theta: {cfg.rope_theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weights to flat binary format\n",
    "import struct\n",
    "\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_LAYERS = 28\n",
    "NUM_Q_HEADS = 16\n",
    "NUM_KV_HEADS = 8\n",
    "HEAD_DIM = 64\n",
    "Q_DIM = NUM_Q_HEADS * HEAD_DIM\n",
    "KV_DIM = NUM_KV_HEADS * HEAD_DIM\n",
    "INTERMEDIATE_DIM = cfg.intermediate_size\n",
    "VOCAB_SIZE = cfg.vocab_size\n",
    "\n",
    "state = model.state_dict()\n",
    "\n",
    "# Print all weight keys for inspection\n",
    "print(\"Weight keys:\")\n",
    "for k, v in state.items():\n",
    "    print(f\"  {k}: {v.shape} {v.dtype}\")\n",
    "\n",
    "# Extract and save weights in our flat binary format\n",
    "parts = []\n",
    "offsets = {}\n",
    "pos = 0\n",
    "\n",
    "def add(name, tensor):\n",
    "    global pos\n",
    "    t = tensor.contiguous().to(torch.bfloat16)\n",
    "    data = t.numpy().view(np.uint16).tobytes()\n",
    "    offsets[name] = pos\n",
    "    parts.append(data)\n",
    "    pos += len(data)\n",
    "    print(f\"  {name}: {tensor.shape} -> {len(data)} bytes (offset {offsets[name]})\")\n",
    "\n",
    "print(\"\\nPacking weights...\")\n",
    "\n",
    "# Embedding\n",
    "add(\"embedding\", state[\"model.embed_tokens.weight\"])\n",
    "\n",
    "# Final norm\n",
    "add(\"final_norm\", state[\"model.norm.weight\"])\n",
    "\n",
    "# Per-layer\n",
    "for i in range(NUM_LAYERS):\n",
    "    prefix = f\"model.layers.{i}\"\n",
    "    add(f\"layer.{i}.attn_norm\", state[f\"{prefix}.input_layernorm.weight\"])\n",
    "    add(f\"layer.{i}.w_q\", state[f\"{prefix}.self_attn.q_proj.weight\"])\n",
    "    add(f\"layer.{i}.w_k\", state[f\"{prefix}.self_attn.k_proj.weight\"])\n",
    "    add(f\"layer.{i}.w_v\", state[f\"{prefix}.self_attn.v_proj.weight\"])\n",
    "    add(f\"layer.{i}.w_o\", state[f\"{prefix}.self_attn.o_proj.weight\"])\n",
    "    add(f\"layer.{i}.ffn_norm\", state[f\"{prefix}.post_attention_layernorm.weight\"])\n",
    "    add(f\"layer.{i}.w_gate\", state[f\"{prefix}.mlp.gate_proj.weight\"])\n",
    "    add(f\"layer.{i}.w_up\", state[f\"{prefix}.mlp.up_proj.weight\"])\n",
    "    add(f\"layer.{i}.w_down\", state[f\"{prefix}.mlp.down_proj.weight\"])\n",
    "\n",
    "# Save binary\n",
    "weights_path = \"weights.bin\"\n",
    "with open(weights_path, \"wb\") as f:\n",
    "    f.write(b\"\".join(parts))\n",
    "\n",
    "offsets_path = \"weights_offsets.json\"\n",
    "with open(offsets_path, \"w\") as f:\n",
    "    json.dump(offsets, f, indent=2)\n",
    "\n",
    "total_mb = pos / 1e6\n",
    "print(f\"\\nSaved {total_mb:.1f} MB to {weights_path}\")\n",
    "print(f\"Offsets saved to {offsets_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Compile Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write source files (if not already present from repo clone)\n",
    "# In production, these come from the git repo. For standalone notebook,\n",
    "# the source files should be uploaded or the repo should be cloned.\n",
    "\n",
    "# Check if source files exist\n",
    "import os\n",
    "if not os.path.exists(\"src/megakernel.cu\"):\n",
    "    print(\"Source files not found! Please clone the repo first:\")\n",
    "    print(\"  !git clone <repo-url>\")\n",
    "    print(\"  Then copy src/ directory here.\")\n",
    "else:\n",
    "    print(\"Source files found.\")\n",
    "\n",
    "# Compile\n",
    "print(f\"\\nCompiling for {GPU_CLASS} ({ARCH})...\")\n",
    "result = subprocess.run(\n",
    "    [\"nvcc\", \"-O3\", f\"-arch={ARCH}\", \"-std=c++17\", \"--use_fast_math\",\n",
    "     \"-lineinfo\", \"-Isrc\", \"src/megakernel.cu\", \"-o\", \"megakernel\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Compilation successful!\")\n",
    "    if result.stderr:\n",
    "        print(f\"Warnings: {result.stderr[:500]}\")\n",
    "else:\n",
    "    print(f\"Compilation FAILED!\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Run standalone info\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n--- Kernel Info ---\")\n",
    "    !./megakernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Correctness Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PyTorch reference forward pass\n",
    "model_gpu = model.to(\"cuda\")\n",
    "model_gpu.eval()\n",
    "\n",
    "# Reference: generate 50 tokens with greedy decoding\n",
    "prompt = \"The meaning of life is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Input IDs: {input_ids[0].tolist()[:10]}...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    ref_output = model_gpu.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "ref_text = tokenizer.decode(ref_output[0], skip_special_tokens=True)\n",
    "ref_tokens = ref_output[0].tolist()\n",
    "print(f\"\\nReference output ({len(ref_tokens)} tokens):\")\n",
    "print(f\"  {ref_text[:300]}\")\n",
    "\n",
    "# Get reference logits for first token\n",
    "with torch.no_grad():\n",
    "    ref_logits = model_gpu(input_ids).logits[0, -1, :]  # [VOCAB_SIZE]\n",
    "    ref_next_token = ref_logits.argmax().item()\n",
    "    print(f\"\\nReference next token: {ref_next_token} = '{tokenizer.decode([ref_next_token])}'\")\n",
    "    print(f\"Top-5 logits: {ref_logits.topk(5)}\")\n",
    "\n",
    "# TODO: Compare with megakernel output once the kernel produces correct results\n",
    "print(\"\\n[Megakernel validation will be added once kernel is verified]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: HuggingFace Baseline Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure HuggingFace baseline throughput\n",
    "print(\"Measuring HuggingFace baseline throughput...\")\n",
    "\n",
    "# Short context (position ~1)\n",
    "input_ids = torch.tensor([[1]], device=\"cuda\")\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(input_ids)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "N = 50\n",
    "start = time.perf_counter()\n",
    "for _ in range(N):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(input_ids)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "hf_toks = N / elapsed\n",
    "\n",
    "print(f\"\\nHuggingFace baseline (short context): {hf_toks:.1f} tok/s\")\n",
    "print(f\"Per-token latency: {1e6/hf_toks:.1f} us\")\n",
    "\n",
    "# Long context baseline\n",
    "long_input = torch.randint(0, cfg.vocab_size, (1, 1024), device=\"cuda\")\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(long_input)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "N_long = 20\n",
    "start = time.perf_counter()\n",
    "for _ in range(N_long):\n",
    "    with torch.no_grad():\n",
    "        model_gpu(long_input)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "hf_long_toks = N_long / elapsed\n",
    "\n",
    "print(f\"HuggingFace baseline (1024 context): {hf_long_toks:.1f} tok/s\")\n",
    "\n",
    "HF_BASELINE = hf_toks\n",
    "print(f\"\\nBaseline to beat: {HF_BASELINE:.1f} tok/s\")\n",
    "print(f\"Target: {THEORETICAL_MAX * 0.71:.0f}+ tok/s (71% bandwidth utilization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Megakernel Benchmarks\n",
    "\n",
    "Once the kernel is fully working, this cell runs the full benchmark suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Full megakernel benchmark\n",
    "# This cell will use ctypes to load the compiled shared library,\n",
    "# set up weight pointers, allocate KV cache and activation buffers,\n",
    "# and run the persistent kernel for benchmarking.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"  MEGAKERNEL BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  GPU: {GPU_NAME}\")\n",
    "print(f\"  Peak BW: {PEAK_BW:.0f} GB/s\")\n",
    "print(f\"  Theoretical max: {THEORETICAL_MAX:.0f} tok/s\")\n",
    "print()\n",
    "\n",
    "# Context position sweep\n",
    "positions = [1, 10, 50, 100, 500, 1000, 2000, 4096]\n",
    "print(f\"{'Position':>10} {'Tok/s':>10} {'Latency(us)':>12} {'BW(GB/s)':>10} {'BW Util':>10} {'vs HF':>8}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "# Placeholder results - replace with actual kernel measurements\n",
    "print(\"[Kernel benchmarks pending - run after kernel correctness is verified]\")\n",
    "print()\n",
    "print(f\"Expected performance at 71% BW utilization:\")\n",
    "expected_toks = THEORETICAL_MAX * 0.71\n",
    "expected_bw = MODEL_BYTES_BF16 * expected_toks / 1e9\n",
    "print(f\"  {expected_toks:.0f} tok/s\")\n",
    "print(f\"  {1e6/expected_toks:.0f} us/token\")\n",
    "print(f\"  {expected_bw:.0f} GB/s effective bandwidth\")\n",
    "print(f\"  {expected_toks/HF_BASELINE:.1f}x faster than HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Text Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation demo using the megakernel\n",
    "# TODO: Replace with actual kernel-based generation\n",
    "\n",
    "prompt = \"Once upon a time in a land far away,\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "\n",
    "# For now, use HuggingFace reference\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    output = model_gpu.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "    )\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "new_tokens = len(output[0]) - len(input_ids[0])\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated {new_tokens} tokens in {elapsed*1000:.1f} ms\")\n",
    "print(f\"Throughput: {new_tokens/elapsed:.1f} tok/s (HuggingFace reference)\")\n",
    "print()\n",
    "print(f\"Output: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Profiling (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Profile with nsys\n",
    "# Requires nsight-systems to be installed (available on Colab)\n",
    "\n",
    "print(\"Profiling is optional. Uncomment to run:\")\n",
    "# !nsys profile --stats=true --output=megakernel_profile ./megakernel\n",
    "# !nsys stats megakernel_profile.nsys-rep\n",
    "\n",
    "# For detailed kernel analysis:\n",
    "# !ncu --set full --launch-count 1 ./megakernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Metric | HuggingFace | MegaQwen | Our Kernel | Theoretical Max |\n",
    "|--------|-------------|----------|------------|------------------|\n",
    "| Tok/s (short ctx) | ~136 | ~530 | TBD | See above |\n",
    "| Tok/s (long ctx) | ~59 | ~158 | TBD | - |\n",
    "| BW Utilization | ~5% | ~5% | Target 71%+ | 100% |\n",
    "| GPU | RTX 3090 | RTX 3090 | A100/H100 | - |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
